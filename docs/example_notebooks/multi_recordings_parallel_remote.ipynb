{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example runs multiple sorters in parallel using remote cluster (srun)\n",
    "# Created by James Jun on Mar 28, 2019\n",
    "\n",
    "# prerequisits\n",
    "# slurm\n",
    "# $ pip install ml_ms4alg\n",
    "# $ conda install -c conda-forge ipywidgets\n",
    "# $ jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "\n",
    "# please ignore the warning when running MountainSort4\n",
    "#   RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeforest_analysis as sa\n",
    "import spikeextractors as se\n",
    "import os, shutil, datetime, json, numpy as np, pandas as pd, json, datetime\n",
    "import sfdata as sf\n",
    "from spikesorters import IronClust, MountainSort4, KiloSort, KiloSort2\n",
    "from spikeforest import spikewidgets as sw\n",
    "import ipywidgets as widgets\n",
    "from spikeforest_analysis.compare_sortings_with_truth import GenSortingComparisonTable\n",
    "from spikeforest_analysis.compute_units_info import ComputeUnitsInfo\n",
    "from matplotlib import pyplot as plt\n",
    "import mlprocessors as mlpr\n",
    "from mlprocessors import LocalComputeResource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sorters\n",
    "# run `createJob` instead of `execute` for delayed execution\n",
    "\n",
    "f_force_run = True\n",
    "adjacency_radius = 100\n",
    "\n",
    "def irc_static(recpath, firings_out):\n",
    "    return IronClust.createJob(\n",
    "            recording_dir=recpath,\n",
    "            firings_out=firings_out,\n",
    "            detect_sign=-1,\n",
    "            adjacency_radius=adjacency_radius,\n",
    "            prm_template_name='static',\n",
    "            _force_run=f_force_run)\n",
    "\n",
    "def irc_drift(recpath, firings_out):\n",
    "    return IronClust.createJob(\n",
    "            recording_dir=recpath,\n",
    "            firings_out=firings_out,\n",
    "            detect_sign=-1,\n",
    "            adjacency_radius=adjacency_radius,\n",
    "            prm_template_name='drift',\n",
    "            _force_run=f_force_run)\n",
    "\n",
    "def ms4(recpath, firings_out):\n",
    "    return MountainSort4.createJob(\n",
    "            recording_dir=recpath,\n",
    "            firings_out=firings_out,\n",
    "            detect_sign=-1,\n",
    "            adjacency_radius=adjacency_radius,\n",
    "            _force_run=f_force_run)\n",
    "\n",
    "def ksort(recpath, firings_out):\n",
    "    return KiloSort.createJob(\n",
    "            recording_dir=recpath,\n",
    "            firings_out=firings_out,\n",
    "            detect_sign=-1,\n",
    "            adjacency_radius=adjacency_radius,\n",
    "            _force_run=f_force_run)\n",
    "\n",
    "def ksort2(recpath, firings_out):\n",
    "    return KiloSort2.createJob(\n",
    "            recording_dir=recpath,\n",
    "            firings_out=firings_out,\n",
    "            detect_sign=-1,\n",
    "            adjacency_radius=adjacency_radius,\n",
    "            _force_run=f_force_run)\n",
    "\n",
    "#D_sorters = dict(KiloSort=ksort, IronClust_static=irc, MountainSort4=ms4, KiloSort2=ksort2, IronClust_drift=irc_drift)\n",
    "D_sorters = dict(KiloSort=ksort, IronClust_static=irc_static, MountainSort4=ms4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a data source and sorter\n",
    "# you may change the data source index and sorter index\n",
    "LS_sorter = list(D_sorters.keys())\n",
    "LS_sorter.insert(0,'all')\n",
    "\n",
    "widget1 = widgets.Dropdown(\n",
    "    options=LS_sorter, index=0, description='Spike sorters')\n",
    "display(widget1)\n",
    "\n",
    "LS_datasource = ['generate locally', 'download']\n",
    "widget2 = widgets.Dropdown(\n",
    "    options = LS_datasource, index=0, description='Data source')\n",
    "display(widget2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a parameters dictionary\n",
    "D_params = dict(\n",
    "    datasource = LS_datasource[widget2.index],\n",
    "    path_in = '/mnt/ceph/users/jjun/temp/recordings/example1',\n",
    "    path_out = '/mnt/ceph/users/jjun/temp/sortings/example1',\n",
    "    num_repeat = 4\n",
    "    )\n",
    "\n",
    "S_sorter = LS_sorter[widget1.index]\n",
    "if S_sorter == 'all':\n",
    "    D_params['sorter'] = LS_sorter[1:]\n",
    "    \n",
    "else:\n",
    "    D_params['sorter'] = [S_sorter]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get recording\n",
    "path_in = D_params['path_in']\n",
    "# delete previous recording\n",
    "#  os.path.exists(recpath): shutil.rmtree(recpath)\n",
    "# delete previous output and make save\n",
    "path_in_true = os.path.join(path_in, 'firings_true.mda')\n",
    "path_in_json = os.path.join(path_in, 'units_info.json')\n",
    "\n",
    "if D_params['datasource'] == 'generate locally':\n",
    "    print(path_in_true)\n",
    "    if not os.path.exists(path_in_true):\n",
    "        # generate recording\n",
    "        rx, sx_true = se.example_datasets.toy_example1(\n",
    "            duration=600, num_channels=4, samplerate=30000, K=10)\n",
    "        print('Generated recording in ' + path_in)\n",
    "    else:\n",
    "        rx = se.MdaRecordingExtractor(path_in)\n",
    "        sx_true = se.MdaSortingExtractor(path_in_true)\n",
    "        print('Using cached recording in ' + path_in)\n",
    "else:\n",
    "    # download recording\n",
    "    #kpath = 'kbucket://15734439d8cf/groundtruth/magland_synth/datasets_noise10_K10_C4/001_synth'\n",
    "    #kpath = '/mnt/home/jjun/ceph/groundtruth/hybrid_drift/rec_32c_600s_11'\n",
    "    kpath = '/mnt/home/jjun/ceph/groundtruth/hybrid_drift/rec_64c_1200s_11'\n",
    "    rx = se.MdaRecordingExtractor(kpath, download=True)\n",
    "    sx_true = se.MdaSortingExtractor(kpath + '/firings_true.mda')   \n",
    "    \n",
    "if not os.path.exists(path_in): \n",
    "    os.makedirs(path_in)\n",
    "    se.MdaRecordingExtractor.writeRecording(recording=rx, save_path=path_in)\n",
    "    se.MdaSortingExtractor.writeSorting(\n",
    "        sorting=sx_true, save_path=path_in_true)\n",
    "        \n",
    "# summarize recording\n",
    "if not os.path.exists(path_in_json):\n",
    "    ComputeUnitsInfo.execute(\n",
    "        recording_dir = path_in,\n",
    "        firings = path_in_true,\n",
    "        json_out = path_in_json,\n",
    "        )\n",
    "with open(path_in_json) as f:\n",
    "    snr_json = json.load(f)\n",
    "    unit_snrs = [x['snr'] for x in snr_json]\n",
    "    unit_ids = [x['unit_id'] for x in snr_json]\n",
    "    sx_true.setUnitsProperty(property_name='snr', \n",
    "         values=unit_snrs, unit_ids=unit_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = datetime.datetime.now()\n",
    "#D_compute = dict(srun_opts='-c 2 -n 80 -p ccm', num_parallel=4)\n",
    "D_compute = dict(srun_opts='-n 1 -c 8 -p gpu --gres=gpu:1 --constraint=v100', num_parallel=6)\n",
    "\n",
    "# Schedule spike sorting tasks\n",
    "path_in = D_params['path_in']\n",
    "path_out = D_params['path_out']\n",
    "nJobs = D_params['num_repeat']\n",
    "L_jobs_sorting = list()\n",
    "for S_sorter1 in D_params['sorter']:\n",
    "    for iJob in range(nJobs):\n",
    "        path_out1 = '{}_{}'.format(os.path.join(path_out, S_sorter1), iJob)\n",
    "        if not os.path.exists(path_out1): \n",
    "            os.makedirs(path_out1)  \n",
    "        path_out_firings1 = os.path.join(path_out1, 'firings_out.mda')\n",
    "        if not os.path.exists(path_out_firings1):\n",
    "            print('Running '+ S_sorter1 + ' in notebook.')\n",
    "            L_jobs_sorting.append(D_sorters[S_sorter1](path_in, path_out_firings1))   \n",
    "        \n",
    "# Run sorting jobs\n",
    "with LocalComputeResource(**D_compute) as compute_resource:\n",
    "    mlpr.executeBatch(jobs=L_jobs_sorting, compute_resource=compute_resource)\n",
    "        \n",
    "# Collect results and run validation\n",
    "L_jobs_comparison = list()\n",
    "for S_sorter1 in D_params['sorter']:\n",
    "    for iJob in range(nJobs):\n",
    "        path_out1 = '{}_{}'.format(os.path.join(path_out, S_sorter1), iJob)\n",
    "        path_out_firings1 = os.path.join(path_out1, 'firings_out.mda')\n",
    "        sx1 = se.MdaSortingExtractor(path_out_firings1)\n",
    "        print('Loaded '+ S_sorter1 + ' from ' + path_out_firings1)\n",
    "    \n",
    "        # validation           \n",
    "        path_out_json1 = os.path.join(path_out1, 'sorting_comparison.json')\n",
    "        if not os.path.exists(path_out_json1):\n",
    "            print('Computing comparison...')\n",
    "            job1 = GenSortingComparisonTable.createJob(\n",
    "                firings = path_out_firings1,\n",
    "                units_true = None,\n",
    "                firings_true = os.path.join(path_in, 'firings_true.mda'),\n",
    "                json_out = path_out_json1,\n",
    "                html_out = os.path.join(path_out1, 'sorting_comparison.html'),\n",
    "                )\n",
    "            L_jobs_comparison.append(job1)\n",
    "\n",
    "# Run comparison jobs\n",
    "with LocalComputeResource(**D_compute) as compute_resource:\n",
    "    mlpr.executeBatch(jobs=L_jobs_comparison, compute_resource=compute_resource)\n",
    "\n",
    "# assemble comparison objects\n",
    "LD_comparison = list()\n",
    "for S_sorter1 in D_params['sorter']:\n",
    "    L_snr = list()\n",
    "    L_accuracy = list()\n",
    "    L_precision = list()\n",
    "    L_recall = list()\n",
    "    for iJob in range(nJobs):\n",
    "        path_out1 = '{}_{}'.format(os.path.join(path_out, S_sorter1), iJob)\n",
    "        path_out_json1 = os.path.join(path_out1, 'sorting_comparison.json')\n",
    "        df1 = pd.read_json(path_or_buf=path_out_json1)    \n",
    "        L_unit_id = [int(x) for x in df1.transpose()['unit_id'].to_list()]\n",
    "        L_accuracy1 = df1.transpose()['accuracy'].to_list()\n",
    "        L_snr1 = sx_true.getUnitsProperty(unit_ids=L_unit_id, property_name='snr')\n",
    "        L_recall1 = [1-x for x in df1.transpose()['f_n'].to_list()]\n",
    "        L_precision1 = [1-x for x in df1.transpose()['f_p'].to_list()]\n",
    "        L_snr.append(L_snr1)  \n",
    "        L_accuracy.append(L_accuracy1)\n",
    "        L_recall.append(L_recall1)\n",
    "        L_precision.append(L_precision1)\n",
    "    D_comparison1 = dict(\n",
    "        L_snr = L_snr, S_sorter=S_sorter1, \n",
    "        L_accuracy = L_accuracy, L_recall = L_recall, L_precision = L_precision)\n",
    "    LD_comparison.append(D_comparison1)\n",
    "    \n",
    "dt = datetime.datetime.now() - t1\n",
    "print('took {:0.3f}s'.format(dt.total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash \n",
    "#rm -fr /mnt/ceph/users/jjun/temp/sortings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot SNR vs accuracy for the first sorting output\n",
    "accuracy_thresh, snr_thresh = .8, 8\n",
    "\n",
    "#LD_comparison1 = [LD_comparison[i] for i in [3,1,4,0,2]]\n",
    "LD_comparison1 = LD_comparison\n",
    "for D_comparison1 in LD_comparison1:\n",
    "    fig=plt.figure(figsize=(12,3))\n",
    "    L_accuracy1 = D_comparison1['L_accuracy']\n",
    "    L_recall1 = D_comparison1['L_recall']\n",
    "    L_precision1 = D_comparison1['L_precision']\n",
    "    L_snr1 = D_comparison1['L_snr']\n",
    "    S_sorter1 = D_comparison1['S_sorter']\n",
    "    \n",
    "    print(S_sorter1)\n",
    "    ax=fig.add_subplot(1,3,1)\n",
    "    ax.plot(L_snr, L_accuracy1, '.')\n",
    "    plt.xlabel('SNR')\n",
    "    plt.ylabel('accuracy')    \n",
    "    plt.ylim(0,1)\n",
    "    nUnits_above = np.sum(np.array(L_accuracy1) >= accuracy_thresh)\n",
    "    plt.title('{} units > {} accuracy'.format(nUnits_above, accuracy_thresh))  \n",
    "    \n",
    "    ax=fig.add_subplot(1,3,2)\n",
    "    ax.plot(L_snr, L_recall1, '.')\n",
    "    plt.xlabel('SNR')\n",
    "    plt.ylabel('recall')    \n",
    "    plt.ylim(0,1)\n",
    "    nUnits_above = np.sum(np.array(L_recall1) >= accuracy_thresh)\n",
    "    plt.title('{} units > {} recall'.format(nUnits_above, accuracy_thresh))  \n",
    "    \n",
    "    ax=fig.add_subplot(1,3,3)\n",
    "    ax.plot(L_snr, L_precision1, '.')\n",
    "    plt.xlabel('SNR')\n",
    "    plt.ylabel('precision')    \n",
    "    plt.ylim(0,1)\n",
    "    nUnits_above = np.sum(np.array(L_precision1) >= accuracy_thresh)\n",
    "    plt.title('{} units > {} precision'.format(nUnits_above, accuracy_thresh))  \n",
    "    \n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
