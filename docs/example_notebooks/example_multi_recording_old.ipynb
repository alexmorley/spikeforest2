{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example runs multiple recording sorting using a local computer\n",
    "# Created by James Jun and Jeremy Magland on Feb 28, 2019\n",
    "\n",
    "# prerequisits\n",
    "# $ pip install ml_ms4alg\n",
    "# $ conda install -c conda-forge ipywidgets\n",
    "# $ jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "\n",
    "# please ignore the warning when running MountainSort4\n",
    "#   RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeforest_analysis as sa\n",
    "from spikeforest import spikeextractors as se\n",
    "import os\n",
    "import shutil\n",
    "import spikeforest as sf\n",
    "import numpy as np\n",
    "from spikesorters import IronClust, MountainSort4\n",
    "from spikeforest import spiketoolkit as st\n",
    "from spikeforest import spikewidgets as sw\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171d67e7ce6c4dc88f577fb33f73c484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Spike sorters', options=('MountainSort4', 'IronClust'), value='MountainSort4')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69552d9846aa4be2bfa09b461c81d5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Data source', options=('generate', 'download'), value='generate')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select a data source and sorter\n",
    "# you may change the data source index and sorter index\n",
    "v_datasource = ['generate', 'download']\n",
    "v_sorter = ['MountainSort4', 'IronClust']\n",
    "\n",
    "widget1 = widgets.Dropdown(\n",
    "    options=v_sorter, \n",
    "    index=0, description='Spike sorters')\n",
    "display(widget1)\n",
    "\n",
    "widget2 = widgets.Dropdown(\n",
    "    options=v_datasource, \n",
    "    index=0, description='Data source')\n",
    "display(widget2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a parameters dictionary\n",
    "\n",
    "params = dict(\n",
    "    sorter = v_sorter[widget1.index],\n",
    "    datasource = v_datasource[widget2.index],\n",
    "    in_path = 'recordings/example1',\n",
    "    out_path = 'sortings/example1',\n",
    "    num_jobs = 4,\n",
    "    num_workers = 4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sorters\n",
    "\n",
    "def irc(recpath):\n",
    "    return IronClust.createJob(\n",
    "            recording_dir=recpath,\n",
    "            firings_out={'ext':'.mda'},\n",
    "            detect_sign=-1,\n",
    "            adjacency_radius=100,\n",
    "            prm_template_name='static',\n",
    "            _force_run=True,\n",
    "            )\n",
    "\n",
    "def ms4(recpath):\n",
    "    return MountainSort4.createJob(\n",
    "            recording_dir=recpath,\n",
    "            firings_out={'ext':'.mda'},\n",
    "            detect_sign=-1,\n",
    "            adjacency_radius=100,\n",
    "            _force_run=True\n",
    "            )\n",
    "\n",
    "v_sorters = dict(IronClust=irc, MountainSort4=ms4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get recording\n",
    "\n",
    "recpath = params['in_path']\n",
    "savepath = params['out_path']\n",
    "\n",
    "# delete previous recording\n",
    "if os.path.exists(recpath): shutil.rmtree(recpath)\n",
    "if not os.path.exists(recpath): os.makedirs(recpath)\n",
    "if not os.path.exists(savepath): os.makedirs(savepath)\n",
    "        \n",
    "if params['datasource'] is 'generate':\n",
    "    # generate recording\n",
    "    rx, sx_true = se.example_datasets.toy_example1(\n",
    "        duration=600, num_channels=4, samplerate=30000, K=10)\n",
    "else:\n",
    "    # download recording\n",
    "    kpath = 'kbucket://15734439d8cf/groundtruth/magland_synth/datasets_noise10_K10_C4/001_synth'\n",
    "    rx = se.MdaRecordingExtractor(kpath, download=True)\n",
    "    sx_true = se.MdaSortingExtractor(kpath + '/firings_true.mda')   \n",
    "    \n",
    "se.MdaRecordingExtractor.writeRecording(\n",
    "    recording=rx, save_path=recpath)\n",
    "se.MdaSortingExtractor.writeSorting(\n",
    "    sorting=sx_true, save_path=os.path.join(savepath, 'firings_true.mda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING: python3 /tmp/tmp0uj_jpd5/execute.py\n",
      "RUNNING: python3 /tmp/tmpi0_4u928/execute.py\n",
      "RUNNING: python3 /tmp/tmpjbf3ovm5/execute.py\n",
      "RUNNING: python3 /tmp/tmp0erc7zcf/execute.py\n",
      "::::::::::::::::::::::::::::: MountainSort4\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/tmp0uj_jpd5/execute.py\", line 8, in <module>\n",
      "    main()\n",
      "  File \"/tmp/tmp0uj_jpd5/execute.py\", line 5, in main\n",
      "    MountainSort4.execute(_cache=True, _force_run=True, _keep_temp_files=None, _container=None, recording_dir='/home/jamesjun/src/spikeforest/docs/example_notebooks/recordings/example1', firings_out='/tmp/tmp0uj_jpd5/output_firings_out.mda', detect_sign=-1, adjacency_radius=100, freq_min=300, freq_max=6000, whiten=True, clip_size=50, detect_threshold=3, detect_interval=10, noise_overlap_threshold=0.15)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/mlprocessors/core.py\", line 505, in execute\n",
      "    return execute(proc, **kwargs)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/mlprocessors/execute.py\", line 797, in execute\n",
      "    stats_signature0 = compute_processor_job_stats_signature(X)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/mlprocessors/execute.py\", line 86, in compute_processor_job_stats_signature\n",
      "    return compute_processor_job_output_signature(self, '--stats--')\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/mlprocessors/execute.py\", line 105, in compute_processor_job_output_signature\n",
      "    val0, input_name=name0, directory=input0.directory),\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/mlprocessors/execute.py\", line 49, in compute_job_input_signature\n",
      "    hash0 = ca.computeDirHash(val)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/cairio/cairioclient.py\", line 193, in computeDirHash\n",
      "    dd = self.readDir(path=path, recursive=True, include_sha1=True)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/cairio/cairioclient.py\", line 189, in readDir\n",
      "    path=path, recursive=recursive, include_sha1=include_sha1)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/cairio/cairioclient.py\", line 419, in _read_file_system_dir\n",
      "    ret['files'][name0]['sha1'] = self.computeFileSha1(path0)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/cairio/cairioclient.py\", line 205, in computeFileSha1\n",
      "    return self._local_db.computeFileSha1(path)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/cairio/cairioclient.py\", line 556, in computeFileSha1\n",
      "    return self._sha1_cache.computeFileSha1(path=path)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/cairio/sha1cache.py\", line 111, in computeFileSha1\n",
      "    path0 = self._get_path(aa_hash, create=True)+'.record.json'\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/cairio/sha1cache.py\", line 159, in _get_path\n",
      "    os.makedirs(path0)\n",
      "  File \"/home/jamesjun/conda/envs/spikeforest/lib/python3.6/os.py\", line 220, in makedirs\n",
      "    mkdir(name, mode)\n",
      "FileExistsError: [Errno 17] File exists: '/tmp/sha1-cache/1/cd'\n",
      "::::::::::::::::::::::::::::: MountainSort4\n",
      "Computing sha1 of /home/jamesjun/src/spikeforest/docs/example_notebooks/recordings/example1/raw.mda\n",
      "MLPR EXECUTING::::::::::::::::::::::::::::: MountainSort4\n",
      "MountainSort4......\n",
      "Using 4 workers.\n",
      "Using tmpdir: /tmp/tmp4__omcrk\n",
      "Num. workers = 4\n",
      "Preparing /tmp/tmp4__omcrk/timeseries.hdf5...\n",
      "Preparing neighborhood sorters (M=4, N=18000000)...\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/site-packages/_pytest/assertion/rewrite.py:8: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "Neighboorhood of channel 0 has 4 channels.\n",
      "Detecting events on channel 1 (phase1)...\n",
      "Neighboorhood of channel 2 has 4 channels.\n",
      "Detecting events on channel 3 (phase1)...\n",
      "Neighboorhood of channel 1 has 4 channels.\n",
      "Detecting events on channel 2 (phase1)...\n",
      "Neighboorhood of channel 3 has 4 channels.\n",
      "::::::::::::::::::::::::::::: MountainSort4\n",
      "Computing sha1 of /home/jamesjun/src/spikeforest/docs/example_notebooks/recordings/example1/raw.mda\n",
      "Detecting events on channel 4 (phase1)...\n",
      "MLPR EXECUTING::::::::::::::::::::::::::::: MountainSort4\n",
      "MountainSort4......\n",
      "Using 4 workers.\n",
      "Using tmpdir: /tmp/tmpt9d4k715\n",
      "::::::::::::::::::::::::::::: MountainSort4\n",
      "Num. workers = 4\n",
      "Preparing /tmp/tmpt9d4k715/timeseries.hdf5...\n",
      "Preparing neighborhood sorters (M=4, N=18000000)...\n",
      "Computing sha1 of /home/jamesjun/src/spikeforest/docs/example_notebooks/recordings/example1/raw.mda\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "MLPR EXECUTING::::::::::::::::::::::::::::: MountainSort4\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "MountainSort4......\n",
      "  return f(*args, **kwds)\n",
      "Using 4 workers.\n",
      "Using tmpdir: /tmp/tmpm15zbdvh\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "Num. workers = 4\n",
      "Preparing /tmp/tmpm15zbdvh/timeseries.hdf5...\n",
      "Preparing neighborhood sorters (M=4, N=18000000)...\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/site-packages/_pytest/assertion/rewrite.py:8: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "  import imp\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "Neighboorhood of channel 0 has 4 channels.\n",
      "  return f(*args, **kwds)\n",
      "Detecting events on channel 1 (phase1)...\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "Neighboorhood of channel 2 has 4 channels.\n",
      "  return f(*args, **kwds)\n",
      "Detecting events on channel 3 (phase1)...\n",
      "/home/jamesjun/conda/envs/spikeforest/lib/python3.6/site-packages/_pytest/assertion/rewrite.py:8: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "Neighboorhood of channel 1 has 4 channels.\n",
      "  import imp\n",
      "Detecting events on channel 2 (phase1)...\n",
      "Neighboorhood of channel 0 has 4 channels.\n",
      "Neighboorhood of channel 3 has 4 channels.\n",
      "Detecting events on channel 1 (phase1)...\n",
      "Detecting events on channel 4 (phase1)...\n",
      "Neighboorhood of channel 2 has 4 channels.\n",
      "Detecting events on channel 3 (phase1)...\n",
      "Neighboorhood of channel 1 has 4 channels.\n",
      "Detecting events on channel 2 (phase1)...\n",
      "Neighboorhood of channel 3 has 4 channels.\n",
      "Detecting events on channel 4 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:08.939188\n",
      "Num events detected on channel 1 (phase1): 7491\n",
      "Computing PCA features for channel 1 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:09.517560\n",
      "Num events detected on channel 4 (phase1): 19515\n",
      "Computing PCA features for channel 4 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:09.466400\n",
      "Num events detected on channel 3 (phase1): 7172\n",
      "Computing PCA features for channel 3 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:09.757672\n",
      "Num events detected on channel 3 (phase1): 19139\n",
      "Computing PCA features for channel 3 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:09.964182\n",
      "Num events detected on channel 2 (phase1): 16727\n",
      "Computing PCA features for channel 2 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:10.047869\n",
      "Num events detected on channel 1 (phase1): 18707\n",
      "Computing PCA features for channel 1 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:10.167949\n",
      "Num events detected on channel 1 (phase1): 9278\n",
      "Computing PCA features for channel 1 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:10.139018\n",
      "Num events detected on channel 3 (phase1): 8128\n",
      "Computing PCA features for channel 3 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:10.370323\n",
      "Num events detected on channel 4 (phase1): 21241\n",
      "Computing PCA features for channel 4 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:10.477203\n",
      "Num events detected on channel 2 (phase1): 8852\n",
      "Elapsed time for detect on neighborhood: 0:00:10.371030\n",
      "Num events detected on channel 4 (phase1): 23444\n",
      "Computing PCA features for channel 2 (phase1)...\n",
      "Computing PCA features for channel 4 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:10.463326\n",
      "Num events detected on channel 2 (phase1): 10110\n",
      "Computing PCA features for channel 2 (phase1)...\n",
      "Clustering for channel 1 (phase1)...\n",
      "Clustering for channel 3 (phase1)...\n",
      "Clustering for channel 1 (phase1)...\n",
      "Clustering for channel 1 (phase1)...\n",
      "Clustering for channel 4 (phase1)...\n",
      "Clustering for channel 2 (phase1)...\n",
      "Clustering for channel 3 (phase1)...\n",
      "Clustering for channel 3 (phase1)...\n",
      "Clustering for channel 4 (phase1)...\n",
      "Clustering for channel 2 (phase1)...\n",
      "Clustering for channel 2 (phase1)...\n",
      "Clustering for channel 4 (phase1)...\n",
      "Found 9 clusters for channel 3 (phase1)...\n",
      "Computing templates for channel 3 (phase1)...\n",
      "Found 10 clusters for channel 1 (phase1)...\n",
      "Found 12 clusters for channel 1 (phase1)...\n",
      "Computing templates for channel 1 (phase1)...\n",
      "Computing templates for channel 1 (phase1)...\n",
      "Found 13 clusters for channel 3 (phase1)...\n",
      "Computing templates for channel 3 (phase1)...\n",
      "Found 14 clusters for channel 2 (phase1)...\n",
      "Computing templates for channel 2 (phase1)...\n",
      "Found 16 clusters for channel 2 (phase1)...\n",
      "Computing templates for channel 2 (phase1)...\n",
      "Found 18 clusters for channel 4 (phase1)...\n",
      "Computing templates for channel 4 (phase1)...\n",
      "Found 19 clusters for channel 2 (phase1)...\n",
      "Computing templates for channel 2 (phase1)...\n",
      "Found 19 clusters for channel 1 (phase1)...\n",
      "Computing templates for channel 1 (phase1)...\n",
      "Found 14 clusters for channel 3 (phase1)...\n",
      "Computing templates for channel 3 (phase1)...\n",
      "Re-assigning events for channel 1 (phase1)...\n",
      "Re-assigning 1 events from 1 to 2 with dt=-3 (k=5)\n",
      "Re-assigning events for channel 3 (phase1)...\n",
      "Re-assigning 1 events from 1 to 3 with dt=-6 (k=8)\n",
      "Re-assigning 1 events from 3 to 4 with dt=-3 (k=3)\n",
      "Re-assigning 1 events from 3 to 1 with dt=-3 (k=5)\n",
      "Re-assigning 113 events from 3 to 4 with dt=2 (k=7)\n",
      "Re-assigning 333 events from 3 to 4 with dt=-1 (k=9)\n",
      "Re-assigning events for channel 1 (phase1)...\n",
      "Re-assigning 1 events from 1 to 4 with dt=-7 (k=4)\n",
      "Re-assigning 2 events from 1 to 2 with dt=-6 (k=5)\n",
      "Re-assigning 1 events from 1 to 2 with dt=-3 (k=8)\n",
      "Re-assigning 1 events from 1 to 3 with dt=-6 (k=11)\n",
      "Found 18 clusters for channel 4 (phase1)...\n",
      "Computing templates for channel 4 (phase1)...\n",
      "Re-assigning events for channel 3 (phase1)...\n",
      "Re-assigning 1 events from 3 to 4 with dt=-3 (k=3)\n",
      "Re-assigning 10 events from 3 to 1 with dt=-2 (k=5)\n",
      "Re-assigning 2 events from 3 to 4 with dt=-2 (k=6)\n",
      "Re-assigning 144 events from 3 to 4 with dt=2 (k=10)\n",
      "Re-assigning 97 events from 3 to 4 with dt=-2 (k=13)\n",
      "Re-assigning events for channel 2 (phase1)...\n",
      "Re-assigning 104 events from 2 to 4 with dt=2 (k=11)\n",
      "Re-assigning events for channel 2 (phase1)...\n",
      "Re-assigning 1 events from 2 to 3 with dt=-3 (k=16)\n",
      "Re-assigning 1 events from 2 to 1 with dt=-1 (k=6)\n",
      "Re-assigning 96 events from 2 to 4 with dt=2 (k=9)\n",
      "Re-assigning events for channel 2 (phase1)...\n",
      "Re-assigning 4 events from 2 to 3 with dt=-2 (k=12)\n",
      "Re-assigning 1 events from 2 to 4 with dt=-6 (k=10)\n",
      "Re-assigning 292 events from 2 to 1 with dt=0 (k=11)\n",
      "Re-assigning 305 events from 2 to 1 with dt=0 (k=18)\n",
      "Re-assigning events for channel 4 (phase1)...\n",
      "Re-assigning 203 events from 4 to 3 with dt=1 (k=8)\n",
      "Re-assigning 5 events from 4 to 1 with dt=-10 (k=13)\n",
      "Re-assigning events for channel 1 (phase1)...\n",
      "Re-assigning 1 events from 1 to 2 with dt=-3 (k=4)\n",
      "Re-assigning 4 events from 1 to 4 with dt=-2 (k=8)\n",
      "Found 16 clusters for channel 4 (phase1)...\n",
      "Computing templates for channel 4 (phase1)...\n",
      "Re-assigning 1 events from 1 to 2 with dt=-3 (k=12)\n",
      "Re-assigning 1 events from 1 to 3 with dt=-6 (k=13)\n",
      "Re-assigning 3 events from 1 to 4 with dt=-9 (k=14)\n",
      "Re-assigning events for channel 3 (phase1)...\n",
      "Re-assigning 30 events from 3 to 1 with dt=-2 (k=7)\n",
      "Re-assigning 9 events from 3 to 1 with dt=-5 (k=9)\n",
      "Re-assigning 24 events from 3 to 4 with dt=-3 (k=13)\n",
      "Neighboorhood of channel 1 has 4 channels.\n",
      "Computing PCA features for channel 2 (phase2)...\n",
      "Neighboorhood of channel 0 has 4 channels.\n",
      "Computing PCA features for channel 1 (phase2)...\n",
      "Neighboorhood of channel 3 has 4 channels.\n",
      "Computing PCA features for channel 4 (phase2)...\n",
      "Neighboorhood of channel 2 has 4 channels.\n",
      "Computing PCA features for channel 3 (phase2)...\n",
      "Re-assigning events for channel 4 (phase1)...\n",
      "Re-assigning 5 events from 4 to 1 with dt=-6 (k=3)\n",
      "Re-assigning 3 events from 4 to 1 with dt=-9 (k=4)\n",
      "Re-assigning 1 events from 4 to 2 with dt=-5 (k=6)\n",
      "Re-assigning 24 events from 4 to 3 with dt=0 (k=7)\n",
      "Re-assigning 7 events from 4 to 3 with dt=-2 (k=8)\n",
      "Neighboorhood of channel 1 has 4 channels.\n",
      "Computing PCA features for channel 2 (phase2)...\n",
      "Neighboorhood of channel 2 has 4 channels.\n",
      "Computing PCA features for channel 3 (phase2)...\n",
      "Neighboorhood of channel 0 has 4 channels.\n",
      "Computing PCA features for channel 1 (phase2)...\n",
      "Neighboorhood of channel 3 has 4 channels.\n",
      "Computing PCA features for channel 4 (phase2)...\n",
      "Re-assigning events for channel 4 (phase1)...\n",
      "Re-assigning 2 events from 4 to 2 with dt=-5 (k=2)\n",
      "Re-assigning 2 events from 4 to 1 with dt=-4 (k=7)\n",
      "Re-assigning 17 events from 4 to 3 with dt=0 (k=8)\n",
      "Re-assigning 6 events from 4 to 3 with dt=-2 (k=9)\n",
      "Neighboorhood of channel 0 has 4 channels.\n",
      "Computing PCA features for channel 1 (phase2)...\n",
      "Neighboorhood of channel 1 has 4 channels.\n",
      "Computing PCA features for channel 2 (phase2)...\n",
      "Neighboorhood of channel 2 has 4 channels.\n",
      "Computing PCA features for channel 3 (phase2)...\n",
      "Neighboorhood of channel 3 has 4 channels.\n",
      "Computing PCA features for channel 4 (phase2)...\n",
      "No duplicate events found for channel 0 in phase2\n",
      "Clustering for channel 1 (phase2)...\n",
      "No duplicate events found for channel 2 in phase2\n",
      "Clustering for channel 3 (phase2)...\n",
      "No duplicate events found for channel 1 in phase2\n",
      "Clustering for channel 2 (phase2)...\n",
      "No duplicate events found for channel 3 in phase2\n",
      "Clustering for channel 4 (phase2)...\n",
      "No duplicate events found for channel 0 in phase2\n",
      "Clustering for channel 1 (phase2)...\n",
      "No duplicate events found for channel 2 in phase2\n",
      "Clustering for channel 3 (phase2)...\n",
      "No duplicate events found for channel 0 in phase2\n",
      "Clustering for channel 1 (phase2)...\n",
      "No duplicate events found for channel 1 in phase2\n",
      "Clustering for channel 2 (phase2)...\n",
      "No duplicate events found for channel 2 in phase2\n",
      "Clustering for channel 3 (phase2)...\n",
      "No duplicate events found for channel 3 in phase2\n",
      "Clustering for channel 4 (phase2)...\n",
      "No duplicate events found for channel 1 in phase2\n",
      "Clustering for channel 2 (phase2)...\n",
      "Found 4 clusters for channel 2 (phase2)...\n",
      "Found 3 clusters for channel 4 (phase2)...\n",
      "Found 4 clusters for channel 1 (phase2)...\n",
      "Found 5 clusters for channel 3 (phase2)...\n",
      "Found 4 clusters for channel 1 (phase2)...\n",
      "No duplicate events found for channel 3 in phase2\n",
      "Clustering for channel 4 (phase2)...\n",
      "Found 7 clusters for channel 3 (phase2)...\n",
      "Preparing output...\n",
      "Done with ms4alg.\n",
      "Found 6 clusters for channel 3 (phase2)...\n",
      "Found 4 clusters for channel 2 (phase2)...\n",
      "Cleaning tmpdir::::: /tmp/tmp4__omcrk\n",
      "MLPR FINISHED ::::::::::::::::::::::::::::: MountainSort4\n",
      "Found 4 clusters for channel 1 (phase2)...\n",
      "Found 3 clusters for channel 2 (phase2)...\n",
      "Found 4 clusters for channel 4 (phase2)...\n",
      "Preparing output...\n",
      "Done with ms4alg.\n",
      "Found 4 clusters for channel 4 (phase2)...\n",
      "Preparing output...\n",
      "Done with ms4alg.\n",
      "Cleaning tmpdir::::: /tmp/tmpt9d4k715\n",
      "MLPR FINISHED ::::::::::::::::::::::::::::: MountainSort4\n",
      "Cleaning tmpdir::::: /tmp/tmpm15zbdvh\n",
      "MLPR FINISHED ::::::::::::::::::::::::::::: MountainSort4\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Non-zero return code when running processor job",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/jamesjun/conda/envs/spikeforest/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/jamesjun/conda/envs/spikeforest/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"/home/jamesjun/src/spikeforest/mountaintools/mlprocessors/execute.py\", line 662, in executeJob\n    raise Exception('Non-zero return code when running processor job')\nException: Non-zero return code when running processor job\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-465ac112194a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmlpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuteBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/src/spikeforest/mountaintools/mlprocessors/execute.py\u001b[0m in \u001b[0;36mexecuteBatch\u001b[0;34m(jobs, label, num_workers, compute_resource, batch_name)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot specify num_workers and compute_resource.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecuteJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/spikeforest/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/spikeforest/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Non-zero return code when running processor job"
     ]
    }
   ],
   "source": [
    "# create a batch (`jobs`) and execute batch\n",
    "\n",
    "import mlprocessors as mlpr\n",
    "\n",
    "jobs=[]\n",
    "for iJob in range(0, params['num_jobs']):\n",
    "    job=v_sorters[params['sorter']](recpath)\n",
    "    jobs.append(job)\n",
    "    \n",
    "\n",
    "mlpr.executeBatch(jobs=jobs, num_workers=params['num_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'command': 'execute_mlprocessor',\n",
       " 'label': 'MountainSort4 (version: 4.2.0) (container: None)',\n",
       " 'processor_name': 'MountainSort4',\n",
       " 'processor_version': '4.2.0',\n",
       " 'processor_class_name': 'MountainSort4',\n",
       " 'processor_code': 'sha1://bd3eca8278d957ce8f8b4583fef7d70da4748783/code.json',\n",
       " 'container': None,\n",
       " 'inputs': {'recording_dir': '/home/jamesjun/src/spikeforest/docs/example_notebooks/recordings/example1'},\n",
       " 'outputs': {'firings_out': {'ext': '.mda'}},\n",
       " 'parameters': {'detect_sign': -1,\n",
       "  'adjacency_radius': 100,\n",
       "  'freq_min': 300,\n",
       "  'freq_max': 6000,\n",
       "  'whiten': True,\n",
       "  'clip_size': 50,\n",
       "  'detect_threshold': 3,\n",
       "  'detect_interval': 10,\n",
       "  'noise_overlap_threshold': 0.15},\n",
       " '_force_run': True,\n",
       " '_cache': True}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fccabd8d502d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mv_sx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miJob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfirings_out_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miJob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'firings_out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMdaSortingExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirings_out_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mv_sx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'result'"
     ]
    }
   ],
   "source": [
    "# assemble sorting outputs\n",
    "\n",
    "v_sx = []\n",
    "for iJob in range(0, len(jobs)):\n",
    "    firings_out_ = jobs[iJob]['result']['outputs']['firings_out']\n",
    "    sx_ = se.MdaSortingExtractor(firings_out_)\n",
    "    v_sx.append(sx_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute SNR of the ground truth units\n",
    "\n",
    "path_json_out = os.path.join(params['out_path'], 'summary_true.mda')\n",
    "sa.compute_units_info.ComputeUnitsInfo.execute(\n",
    "    recording_dir = params['in_path'],\n",
    "    firings = os.path.join(params['out_path'], 'firings_true.mda'),\n",
    "    json_out = path_json_out,\n",
    "    _force_run = True\n",
    "    )\n",
    "\n",
    "import json\n",
    "with open(path_json_out) as f:\n",
    "    snr_json = json.load(f)\n",
    "unit_snrs = [x['snr'] for x in snr_json]\n",
    "unit_ids = [x['unit_id'] for x in snr_json]\n",
    "sx_true.setUnitsProperty(property_name='snr', \n",
    "                         values=unit_snrs, unit_ids=unit_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display sorting comparison tables\n",
    "\n",
    "v_comparison = []\n",
    "for iJob in range(0, len(v_sx)):\n",
    "    sx_ = v_sx[iJob]\n",
    "    comparison=st.comparison.SortingComparison(\n",
    "        sorting1=sx_true, sorting1_name='true',\n",
    "        sorting2=sx_, sorting2_name=params['sorter'],\n",
    "        )\n",
    "    comparison_table = sw.SortingComparisonTable(comparison=comparison)\n",
    "    v_comparison.append(comparison)\n",
    "    \n",
    "    print('sorting output for {} in job {}:'.format(\n",
    "        params['sorter'], iJob))\n",
    "    comparison_table.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot SNR vs accuracy for the first sorting output\n",
    "\n",
    "sw.SortingAccuracyWidget(\n",
    "    sorting_comparison=v_comparison[0],\n",
    "    property_name='snr',\n",
    "    ).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot SNR vs accuracy for all sorting output\n",
    "\n",
    "def plot_comparisons(v_comparison, params):\n",
    "    v_snr = []\n",
    "    v_accuracy = []\n",
    "    for iJob in range(0, len(v_comparison)):\n",
    "    #for iJob in range(1,2):\n",
    "        SC = v_comparison[iJob]\n",
    "        units = SC.getSorting1().getUnitIds()\n",
    "        accuracy = [SC.getAgreementFraction(unit) for unit in units]\n",
    "        snr = SC.getSorting1().getUnitsProperty(unit_ids=units, property_name='snr')\n",
    "        v_snr.append(snr)\n",
    "        v_accuracy.append(accuracy)\n",
    "\n",
    "    plt.plot(v_snr, v_accuracy, '.')\n",
    "    plt.xlabel('SNR')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Sorted by {} on {} recordings'.format(\n",
    "        params['sorter'], params['num_jobs']))\n",
    "    plt.ylim(0,1)\n",
    "    plt.show()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparisons(v_comparison, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparisons(v_comparison, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
