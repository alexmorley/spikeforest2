{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example runs multiple recording sorting using a local computer\n",
    "# Created by James Jun and Jeremy Magland on Feb 28, 2019\n",
    "\n",
    "# prerequisits\n",
    "# $ pip install ml_ms4alg\n",
    "# $ conda install -c conda-forge ipywidgets\n",
    "# $ jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "\n",
    "# please ignore the warning when running MountainSort4\n",
    "#   RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeforest_analysis as sa\n",
    "import spikeextractors as se\n",
    "import os\n",
    "import shutil\n",
    "import spikeforest as sf\n",
    "import numpy as np\n",
    "from spikesorters import IronClust, MountainSort4\n",
    "from spikeforest import SortingComparison\n",
    "from spikeforest import spikewidgets as sw\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a data source and sorter\n",
    "# you may change the data source index and sorter index\n",
    "v_datasource = ['generate locally', 'download', 'local']\n",
    "v_sorter = ['MountainSort4', 'IronClust']\n",
    "\n",
    "widget1 = widgets.Dropdown(\n",
    "    options=v_sorter, \n",
    "    index=0, description='Spike sorters')\n",
    "display(widget1)\n",
    "\n",
    "widget2 = widgets.Dropdown(\n",
    "    options=v_datasource, \n",
    "    index=0, description='Data source')\n",
    "display(widget2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a parameters dictionary\n",
    "\n",
    "params = dict(\n",
    "    sorter = v_sorter[widget1.index],\n",
    "    datasource = v_datasource[widget2.index],\n",
    "    in_path = 'recordings/example1',\n",
    "    out_path = 'sortings/example1',\n",
    "    num_jobs = 4,\n",
    "    num_workers = 2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sorters\n",
    "\n",
    "def irc(recpath, sortpath):\n",
    "    return IronClust.createJob(\n",
    "            recording_dir=recpath,\n",
    "            firings_out=os.path.join(sortpath, 'firings_out.mda'),\n",
    "            detect_sign=-1,\n",
    "            adjacency_radius=100,\n",
    "            prm_template_name='static',\n",
    "            _force_run=True,\n",
    "            )\n",
    "\n",
    "def ms4(recpath, sortpath):\n",
    "    return MountainSort4.createJob(\n",
    "            recording_dir=recpath,\n",
    "            firings_out=os.path.join(sortpath, 'firings_out.mda'),\n",
    "            detect_sign=-1,\n",
    "            adjacency_radius=100,\n",
    "            _force_run=True\n",
    "            )\n",
    "\n",
    "v_sorters = dict(IronClust=irc, MountainSort4=ms4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get recording\n",
    "\n",
    "recpath = params['in_path']\n",
    "savepath = params['out_path']\n",
    "\n",
    "# delete previous recording\n",
    "if os.path.exists(recpath): shutil.rmtree(recpath)\n",
    "if not os.path.exists(recpath): os.makedirs(recpath)\n",
    "if not os.path.exists(savepath): os.makedirs(savepath)\n",
    "        \n",
    "if params['datasource'] is 'generate locally':\n",
    "    # generate recording\n",
    "    rx, sx_true = se.example_datasets.toy_example1(\n",
    "        duration=600, num_channels=4, samplerate=30000, K=10)\n",
    "elif params['datasource'] is 'download':\n",
    "    # download recording\n",
    "    kpath = 'kbucket://15734439d8cf/groundtruth/magland_synth/datasets_noise10_K10_C4/001_synth'\n",
    "    rx = se.MdaRecordingExtractor(kpath, download=True)\n",
    "    sx_true = se.MdaSortingExtractor(kpath + '/firings_true.mda')   \n",
    "else:\n",
    "    local_path = '/mnt/k/spikeforest/groundtruth/magland_synth/datasets_noise10_K10_C4/001_synth'\n",
    "    #local_path = '/home/jamesjun/example1'\n",
    "    rx = se.MdaRecordingExtractor(local_path, download=True)\n",
    "    sx_true = se.MdaSortingExtractor(local_path + '/firings_true.mda')     \n",
    "    \n",
    "se.MdaRecordingExtractor.write_recording(\n",
    "    recording=rx, save_path=recpath)\n",
    "se.MdaSortingExtractor.write_sorting(\n",
    "    sorting=sx_true, save_path=os.path.join(savepath, 'firings_true.mda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING: python3 /tmp/tmpowzrqq4z/execute.py\n",
      "RUNNING: python3 /tmp/tmpetqmpy1w/execute.py\n",
      "RUNNING: python3 /tmp/tmpwxyk_a9f/execute.py\n",
      "RUNNING: python3 /tmp/tmpz95qxrhb/execute.py\n",
      "::::::::::::::::::::::::::::: MountainSort4\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/tmpz95qxrhb/execute.py\", line 8, in <module>\n",
      "    main()\n",
      "  File \"/tmp/tmpz95qxrhb/execute.py\", line 5, in main\n",
      "    MountainSort4.execute(_cache=True, _force_run=True, _keep_temp_files=None, _container=None, recording_dir='/home/jamesjun/src/spikeforest/docs/example_notebooks/recordings/example1', firings_out='/tmp/tmpz95qxrhb/output_firings_out.mda', detect_sign=-1, adjacency_radius=100, freq_min=300, freq_max=6000, whiten=True, clip_size=50, detect_threshold=3, detect_interval=10, noise_overlap_threshold=0.15)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/mlprocessors/core.py\", line 505, in execute\n",
      "    return execute(proc, **kwargs)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/mlprocessors/execute.py\", line 797, in execute\n",
      "    stats_signature0 = compute_processor_job_stats_signature(X)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/mlprocessors/execute.py\", line 86, in compute_processor_job_stats_signature\n",
      "    return compute_processor_job_output_signature(self, '--stats--')\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/mlprocessors/execute.py\", line 105, in compute_processor_job_output_signature\n",
      "    val0, input_name=name0, directory=input0.directory),\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/mlprocessors/execute.py\", line 49, in compute_job_input_signature\n",
      "    hash0 = ca.computeDirHash(val)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/cairio/cairioclient.py\", line 193, in computeDirHash\n",
      "    dd = self.readDir(path=path, recursive=True, include_sha1=True)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/cairio/cairioclient.py\", line 189, in readDir\n",
      "    path=path, recursive=recursive, include_sha1=include_sha1)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/cairio/cairioclient.py\", line 419, in _read_file_system_dir\n",
      "    ret['files'][name0]['sha1'] = self.computeFileSha1(path0)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/cairio/cairioclient.py\", line 205, in computeFileSha1\n",
      "    return self._local_db.computeFileSha1(path)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/cairio/cairioclient.py\", line 556, in computeFileSha1\n",
      "    return self._sha1_cache.computeFileSha1(path=path)\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/cairio/sha1cache.py\", line 111, in computeFileSha1\n",
      "    path0 = self._get_path(aa_hash, create=True)+'.record.json'\n",
      "  File \"/home/jamesjun/src/spikeforest/mountaintools/cairio/sha1cache.py\", line 159, in _get_path\n",
      "    os.makedirs(path0)\n",
      "  File \"/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/os.py\", line 220, in makedirs\n",
      "    mkdir(name, mode)\n",
      "FileExistsError: [Errno 17] File exists: '/tmp/sha1-cache/b/b2'\n",
      "::::::::::::::::::::::::::::: MountainSort4\n",
      "Warning: Unable to read or parse json file: /tmp/sha1-cache/b/b2/bb202b3bcfa7867c600c7338c4c64e3084564264.record.json\n",
      "Computing sha1 of /home/jamesjun/src/spikeforest/docs/example_notebooks/recordings/example1/raw.mda\n",
      "Found output \"firings_out\" in cache.\n",
      "Found outputs in cache, but forcing run...\n",
      "MLPR EXECUTING::::::::::::::::::::::::::::: MountainSort4\n",
      "MountainSort4......\n",
      "Using 20 workers.\n",
      "Using tmpdir: /tmp/tmpssgf7z6i\n",
      "Num. workers = 20\n",
      "Preparing /tmp/tmpssgf7z6i/timeseries.hdf5...\n",
      "Preparing neighborhood sorters (M=4, N=18000000)...\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/site-packages/_pytest/assertion/rewrite.py:8: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "::::::::::::::::::::::::::::: MountainSort4\n",
      "Computing sha1 of /home/jamesjun/src/spikeforest/docs/example_notebooks/recordings/example1/raw.mda\n",
      "Found output \"firings_out\" in cache.\n",
      "Found outputs in cache, but forcing run...\n",
      "MLPR EXECUTING::::::::::::::::::::::::::::: MountainSort4\n",
      "MountainSort4......\n",
      "Using 20 workers.\n",
      "Using tmpdir: /tmp/tmp1px6fq8p\n",
      "Num. workers = 20\n",
      "Preparing /tmp/tmp1px6fq8p/timeseries.hdf5...\n",
      "Preparing neighborhood sorters (M=4, N=18000000)...\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/site-packages/_pytest/assertion/rewrite.py:8: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "::::::::::::::::::::::::::::: MountainSort4\n",
      "Computing sha1 of /home/jamesjun/src/spikeforest/docs/example_notebooks/recordings/example1/raw.mda\n",
      "Found output \"firings_out\" in cache.\n",
      "Found outputs in cache, but forcing run...\n",
      "MLPR EXECUTING::::::::::::::::::::::::::::: MountainSort4\n",
      "MountainSort4......\n",
      "Using 20 workers.\n",
      "Using tmpdir: /tmp/tmpk06sysou\n",
      "Num. workers = 20\n",
      "Preparing /tmp/tmpk06sysou/timeseries.hdf5...\n",
      "Preparing neighborhood sorters (M=4, N=18000000)...\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/site-packages/_pytest/assertion/rewrite.py:8: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "Neighboorhood of channel 1 has 4 channels.\n",
      "Detecting events on channel 2 (phase1)...\n",
      "Neighboorhood of channel 0 has 4 channels.\n",
      "Detecting events on channel 1 (phase1)...\n",
      "Neighboorhood of channel 2 has 4 channels.\n",
      "Detecting events on channel 3 (phase1)...\n",
      "Neighboorhood of channel 3 has 4 channels.\n",
      "Detecting events on channel 4 (phase1)...\n",
      "Neighboorhood of channel 0 has 4 channels.\n",
      "Detecting events on channel 1 (phase1)...\n",
      "Neighboorhood of channel 1 has 4 channels.\n",
      "Detecting events on channel 2 (phase1)...\n",
      "Neighboorhood of channel 2 has 4 channels.\n",
      "Detecting events on channel 3 (phase1)...\n",
      "Neighboorhood of channel 3 has 4 channels.\n",
      "Detecting events on channel 4 (phase1)...\n",
      "Neighboorhood of channel 0 has 4 channels.\n",
      "Detecting events on channel 1 (phase1)...\n",
      "Neighboorhood of channel 1 has 4 channels.\n",
      "Detecting events on channel 2 (phase1)...\n",
      "Neighboorhood of channel 2 has 4 channels.\n",
      "Detecting events on channel 3 (phase1)...\n",
      "Neighboorhood of channel 3 has 4 channels.\n",
      "Detecting events on channel 4 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:02.813781\n",
      "Num events detected on channel 4 (phase1): 13726\n",
      "Elapsed time for detect on neighborhood: 0:00:02.814862\n",
      "Num events detected on channel 2 (phase1): 18835\n",
      "Computing PCA features for channel 4 (phase1)...\n",
      "Computing PCA features for channel 2 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:02.910870\n",
      "Num events detected on channel 1 (phase1): 12517\n",
      "Computing PCA features for channel 1 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:02.943013\n",
      "Num events detected on channel 3 (phase1): 15214\n",
      "Computing PCA features for channel 3 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:03.119756\n",
      "Num events detected on channel 2 (phase1): 17289\n",
      "Computing PCA features for channel 2 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:03.241062\n",
      "Num events detected on channel 4 (phase1): 26380\n",
      "Computing PCA features for channel 4 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:03.312874\n",
      "Num events detected on channel 3 (phase1): 17724\n",
      "Computing PCA features for channel 3 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:03.337460\n",
      "Num events detected on channel 1 (phase1): 17597\n",
      "Computing PCA features for channel 1 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:03.725321\n",
      "Num events detected on channel 1 (phase1): 12257\n",
      "Computing PCA features for channel 1 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:03.743651\n",
      "Num events detected on channel 2 (phase1): 18234\n",
      "Computing PCA features for channel 2 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:03.748524\n",
      "Num events detected on channel 4 (phase1): 17258\n",
      "Computing PCA features for channel 4 (phase1)...\n",
      "Elapsed time for detect on neighborhood: 0:00:03.795105\n",
      "Num events detected on channel 3 (phase1): 17883\n",
      "Computing PCA features for channel 3 (phase1)...\n",
      "Clustering for channel 4 (phase1)...\n",
      "Clustering for channel 2 (phase1)...\n",
      "Clustering for channel 1 (phase1)...\n",
      "Clustering for channel 3 (phase1)...\n",
      "Found 11 clusters for channel 4 (phase1)...\n",
      "Computing templates for channel 4 (phase1)...\n",
      "Found 12 clusters for channel 1 (phase1)...\n",
      "Computing templates for channel 1 (phase1)...\n",
      "Found 11 clusters for channel 3 (phase1)...\n",
      "Computing templates for channel 3 (phase1)...\n",
      "Found 11 clusters for channel 2 (phase1)...\n",
      "Computing templates for channel 2 (phase1)...\n",
      "Clustering for channel 2 (phase1)...\n",
      "Clustering for channel 1 (phase1)...\n",
      "Clustering for channel 3 (phase1)...\n",
      "Clustering for channel 4 (phase1)...\n",
      "Re-assigning events for channel 4 (phase1)...\n",
      "Re-assigning 39 events from 4 to 2 with dt=-5 (k=2)\n",
      "Re-assigning 11 events from 4 to 3 with dt=-1 (k=3)\n",
      "Re-assigning 2 events from 4 to 1 with dt=-5 (k=7)\n",
      "Re-assigning 1 events from 4 to 3 with dt=2 (k=8)\n",
      "Clustering for channel 1 (phase1)...\n",
      "Clustering for channel 3 (phase1)...\n",
      "Clustering for channel 2 (phase1)...\n",
      "Clustering for channel 4 (phase1)...\n",
      "Re-assigning events for channel 1 (phase1)...\n",
      "Re-assigning 10 events from 1 to 2 with dt=-3 (k=1)\n",
      "Re-assigning 3 events from 1 to 3 with dt=-5 (k=7)\n",
      "Found 11 clusters for channel 1 (phase1)...\n",
      "Computing templates for channel 1 (phase1)...\n",
      "Re-assigning events for channel 2 (phase1)...\n",
      "Re-assigning 44 events from 2 to 3 with dt=-4 (k=3)\n",
      "Re-assigning 8 events from 2 to 1 with dt=1 (k=4)\n",
      "Re-assigning 15 events from 2 to 3 with dt=-3 (k=6)\n",
      "Found 11 clusters for channel 2 (phase1)...\n",
      "Computing templates for channel 2 (phase1)...\n",
      "Re-assigning events for channel 3 (phase1)...\n",
      "Re-assigning 218 events from 3 to 2 with dt=-1 (k=1)\n",
      "Re-assigning 1 events from 3 to 1 with dt=-7 (k=3)\n",
      "Re-assigning 44 events from 3 to 1 with dt=-2 (k=5)\n",
      "Found 12 clusters for channel 3 (phase1)...\n",
      "Computing templates for channel 3 (phase1)...\n",
      "Found 11 clusters for channel 4 (phase1)...\n",
      "Computing templates for channel 4 (phase1)...\n",
      "Found 11 clusters for channel 1 (phase1)...\n",
      "Computing templates for channel 1 (phase1)...\n",
      "Found 11 clusters for channel 3 (phase1)...\n",
      "Computing templates for channel 3 (phase1)...\n",
      "Found 11 clusters for channel 2 (phase1)...\n",
      "Computing templates for channel 2 (phase1)...\n",
      "Found 11 clusters for channel 4 (phase1)...\n",
      "Computing templates for channel 4 (phase1)...\n",
      "Re-assigning events for channel 1 (phase1)...\n",
      "Re-assigning 18 events from 1 to 2 with dt=-3 (k=2)\n",
      "Re-assigning 4 events from 1 to 2 with dt=-3 (k=8)\n",
      "Re-assigning 3 events from 1 to 3 with dt=-5 (k=9)\n",
      "Re-assigning events for channel 2 (phase1)...\n",
      "Re-assigning 13 events from 2 to 3 with dt=-4 (k=3)\n",
      "Re-assigning 316 events from 2 to 3 with dt=-1 (k=4)\n",
      "Re-assigning 1 events from 2 to 1 with dt=1 (k=6)\n",
      "Re-assigning 2 events from 2 to 3 with dt=-3 (k=7)\n",
      "Re-assigning events for channel 3 (phase1)...\n",
      "Re-assigning 1 events from 3 to 1 with dt=-7 (k=3)\n",
      "Re-assigning 25 events from 3 to 1 with dt=-2 (k=8)\n",
      "Re-assigning 6 events from 3 to 2 with dt=-3 (k=10)\n",
      "Re-assigning events for channel 4 (phase1)...\n",
      "Re-assigning 193 events from 4 to 2 with dt=-5 (k=2)\n",
      "Re-assigning 71 events from 4 to 3 with dt=-1 (k=3)\n",
      "Re-assigning 3 events from 4 to 1 with dt=-5 (k=6)\n",
      "Re-assigning 1 events from 4 to 2 with dt=-7 (k=7)\n",
      "Re-assigning 7 events from 4 to 3 with dt=2 (k=8)\n",
      "Re-assigning events for channel 1 (phase1)...\n",
      "Re-assigning 5 events from 1 to 2 with dt=-3 (k=1)\n",
      "Re-assigning 1 events from 1 to 3 with dt=-5 (k=5)\n",
      "Re-assigning 3 events from 1 to 2 with dt=-3 (k=8)\n",
      "Re-assigning events for channel 3 (phase1)...\n",
      "Re-assigning 348 events from 3 to 2 with dt=-1 (k=1)\n",
      "Re-assigning 2 events from 3 to 1 with dt=-7 (k=3)\n",
      "Re-assigning 103 events from 3 to 1 with dt=-2 (k=5)\n",
      "Re-assigning 2 events from 3 to 2 with dt=-3 (k=6)\n",
      "Re-assigning 4 events from 3 to 2 with dt=-3 (k=7)\n",
      "Re-assigning events for channel 2 (phase1)...\n",
      "Re-assigning 24 events from 2 to 3 with dt=-4 (k=3)\n",
      "Re-assigning 2 events from 2 to 3 with dt=-3 (k=4)\n",
      "Re-assigning 2 events from 2 to 1 with dt=1 (k=6)\n",
      "Re-assigning 340 events from 2 to 3 with dt=-1 (k=7)\n",
      "Re-assigning events for channel 4 (phase1)...\n",
      "Re-assigning 48 events from 4 to 2 with dt=-5 (k=2)\n",
      "Re-assigning 18 events from 4 to 3 with dt=-1 (k=3)\n",
      "Re-assigning 4 events from 4 to 1 with dt=-5 (k=9)\n",
      "Re-assigning 1 events from 4 to 2 with dt=-6 (k=10)\n",
      "Neighboorhood of channel 3 has 4 channels.\n",
      "Computing PCA features for channel 4 (phase2)...\n",
      "Neighboorhood of channel 0 has 4 channels.\n",
      "Computing PCA features for channel 1 (phase2)...\n",
      "Neighboorhood of channel 1 has 4 channels.\n",
      "Computing PCA features for channel 2 (phase2)...\n",
      "Neighboorhood of channel 2 has 4 channels.\n",
      "Computing PCA features for channel 3 (phase2)...\n",
      "No duplicate events found for channel 3 in phase2\n",
      "Clustering for channel 4 (phase2)...\n",
      "No duplicate events found for channel 1 in phase2\n",
      "Clustering for channel 2 (phase2)...\n",
      "Neighboorhood of channel 0 has 4 channels.\n",
      "Computing PCA features for channel 1 (phase2)...\n",
      "Neighboorhood of channel 3 has 4 channels.\n",
      "Computing PCA features for channel 4 (phase2)...\n",
      "Neighboorhood of channel 1 has 4 channels.\n",
      "Computing PCA features for channel 2 (phase2)...\n",
      "Neighboorhood of channel 2 has 4 channels.\n",
      "Computing PCA features for channel 3 (phase2)...\n",
      "No duplicate events found for channel 0 in phase2\n",
      "Clustering for channel 1 (phase2)...\n",
      "Found 2 clusters for channel 4 (phase2)...\n",
      "No duplicate events found for channel 2 in phase2\n",
      "Clustering for channel 3 (phase2)...\n",
      "Found 3 clusters for channel 1 (phase2)...\n",
      "Found 5 clusters for channel 2 (phase2)...\n",
      "Neighboorhood of channel 1 has 4 channels.\n",
      "Computing PCA features for channel 2 (phase2)...\n",
      "Neighboorhood of channel 0 has 4 channels.\n",
      "Computing PCA features for channel 1 (phase2)...\n",
      "Neighboorhood of channel 3 has 4 channels.\n",
      "Computing PCA features for channel 4 (phase2)...\n",
      "Neighboorhood of channel 2 has 4 channels.\n",
      "Computing PCA features for channel 3 (phase2)...\n",
      "Found 5 clusters for channel 3 (phase2)...\n",
      "Preparing output...\n",
      "Done with ms4alg.\n",
      "Cleaning tmpdir::::: /tmp/tmpssgf7z6i\n",
      "MLPR FINISHED ::::::::::::::::::::::::::::: MountainSort4\n",
      "No duplicate events found for channel 3 in phase2\n",
      "Clustering for channel 4 (phase2)...\n",
      "No duplicate events found for channel 0 in phase2\n",
      "Clustering for channel 1 (phase2)...\n",
      "No duplicate events found for channel 1 in phase2\n",
      "Clustering for channel 2 (phase2)...\n",
      "No duplicate events found for channel 2 in phase2\n",
      "Clustering for channel 3 (phase2)...\n",
      "Found 3 clusters for channel 1 (phase2)...\n",
      "Found 4 clusters for channel 2 (phase2)...\n",
      "No duplicate events found for channel 0 in phase2\n",
      "Clustering for channel 1 (phase2)...\n",
      "No duplicate events found for channel 3 in phase2\n",
      "Clustering for channel 4 (phase2)...\n",
      "Found 2 clusters for channel 4 (phase2)...\n",
      "No duplicate events found for channel 1 in phase2\n",
      "Clustering for channel 2 (phase2)...\n",
      "No duplicate events found for channel 2 in phase2\n",
      "Clustering for channel 3 (phase2)...\n",
      "Found 3 clusters for channel 1 (phase2)...\n",
      "Found 6 clusters for channel 3 (phase2)...\n",
      "Found 2 clusters for channel 4 (phase2)...\n",
      "Preparing output...\n",
      "Done with ms4alg.\n",
      "Cleaning tmpdir::::: /tmp/tmp1px6fq8p\n",
      "MLPR FINISHED ::::::::::::::::::::::::::::: MountainSort4\n",
      "Found 4 clusters for channel 2 (phase2)...\n",
      "Found 5 clusters for channel 3 (phase2)...\n",
      "Preparing output...\n",
      "Done with ms4alg.\n",
      "Cleaning tmpdir::::: /tmp/tmpk06sysou\n",
      "MLPR FINISHED ::::::::::::::::::::::::::::: MountainSort4\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Non-zero return code when running processor job",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/jamesjun/miniconda3/envs/spikeforest/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"/home/jamesjun/src/spikeforest/mountaintools/mlprocessors/execute.py\", line 662, in executeJob\n    raise Exception('Non-zero return code when running processor job')\nException: Non-zero return code when running processor job\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-588938359f64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#%time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmlpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuteBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/src/spikeforest/mountaintools/mlprocessors/execute.py\u001b[0m in \u001b[0;36mexecuteBatch\u001b[0;34m(jobs, label, num_workers, compute_resource, batch_name)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot specify num_workers and compute_resource.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecuteJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/spikeforest/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/spikeforest/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Non-zero return code when running processor job"
     ]
    }
   ],
   "source": [
    "# create a batch (`jobs`) and execute batch\n",
    "\n",
    "import mlprocessors as mlpr\n",
    "\n",
    "jobs=[]\n",
    "v_savepath=[]\n",
    "for iJob in range(0, params['num_jobs']):\n",
    "    savepath1=os.path.join(savepath, '{}_{}'.format(params['sorter'], iJob))\n",
    "    if not os.path.exists(savepath1): os.makedirs(savepath1)\n",
    "    job=v_sorters[params['sorter']](recpath, savepath1)\n",
    "    jobs.append(job)\n",
    "    v_savepath.append(savepath1)\n",
    "\n",
    "#%time \n",
    "mlpr.executeBatch(jobs=jobs, num_workers=params['num_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble sorting outputs\n",
    "\n",
    "v_sx = []\n",
    "for iJob in range(0, len(jobs)):\n",
    "    # firings_out_ = jobs[iJob]['result']['outputs']['firings_out']\n",
    "    firings_out1 = os.path.join(v_savepath[iJob], 'firings_out.mda')\n",
    "    sx_ = se.MdaSortingExtractor(firings_out1)\n",
    "    v_sx.append(sx_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute SNR of the ground truth units\n",
    "\n",
    "path_json_out = os.path.join(params['out_path'], 'summary_true.mda')\n",
    "sa.compute_units_info.ComputeUnitsInfo.execute(\n",
    "    recording_dir = params['in_path'],\n",
    "    firings = os.path.join(params['out_path'], 'firings_true.mda'),\n",
    "    json_out = path_json_out,\n",
    "    _force_run = True\n",
    "    )\n",
    "\n",
    "import json\n",
    "with open(path_json_out) as f:\n",
    "    snr_json = json.load(f)\n",
    "unit_snrs = [x['snr'] for x in snr_json]\n",
    "unit_ids = [x['unit_id'] for x in snr_json]\n",
    "sx_true.setUnitsProperty(property_name='snr', \n",
    "                         values=unit_snrs, unit_ids=unit_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display sorting comparison tables\n",
    "\n",
    "v_comparison = []\n",
    "for iJob in range(0, len(v_sx)):\n",
    "    sx_ = v_sx[iJob]\n",
    "    comparison=SortingComparison(\n",
    "        sorting1=sx_true, sorting1_name='true',\n",
    "        sorting2=sx_, sorting2_name=params['sorter'],\n",
    "        )\n",
    "    comparison_table = sw.SortingComparisonTable(comparison=comparison)\n",
    "    v_comparison.append(comparison)\n",
    "    \n",
    "    print('sorting output for {} in job {}:'.format(\n",
    "        params['sorter'], iJob))\n",
    "    comparison_table.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot SNR vs accuracy for the first sorting output\n",
    "\n",
    "sw.SortingAccuracyWidget(\n",
    "    sorting_comparison=v_comparison[0],\n",
    "    property_name='snr',\n",
    "    ).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot SNR vs accuracy for all sorting output\n",
    "\n",
    "def plot_comparisons(v_comparison, params):\n",
    "    v_snr = []\n",
    "    v_accuracy = []\n",
    "    for iJob in range(0, len(v_comparison)):\n",
    "    #for iJob in range(1,2):\n",
    "        SC = v_comparison[iJob]\n",
    "        units = SC.getSorting1().get_unit_ids()\n",
    "        accuracy = [SC.getAgreementFraction(unit) for unit in units]\n",
    "        snr = SC.getSorting1().getUnitsProperty(unit_ids=units, property_name='snr')\n",
    "        v_snr.append(snr)\n",
    "        v_accuracy.append(accuracy)\n",
    "\n",
    "    plt.plot(v_snr, v_accuracy, '.')\n",
    "    plt.xlabel('SNR')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Sorted by {} on {} recordings'.format(\n",
    "        params['sorter'], params['num_jobs']))\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlim(2, 20)\n",
    "    plt.show()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparisons(v_comparison, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparisons(v_comparison, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
